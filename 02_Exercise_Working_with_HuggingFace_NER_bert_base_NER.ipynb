{"cells":[{"cell_type":"markdown","source":["\n","### Named Entity Recognition (NER) by directly using the bert-base-NER model in Hugging Face\n"],"metadata":{"id":"P-gOptb-RoTE"}},{"cell_type":"markdown","source":["# Install Transformers and Datasets from Hugging Face"],"metadata":{"id":"FtxOv_uTSC4m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3Kf0jYlkd9c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3460ad9b-b9b7-47fc-f299-bc492e2b52d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Collecting datasets\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/121.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:32\u001b[0m"]}],"source":["# Transformers installation\n","! pip install transformers[torch] datasets\n","# To install from source instead of the last release, comment the command above and uncomment the following one.\n","# ! pip install git+https://github.com/huggingface/transformers.git"]},{"cell_type":"markdown","metadata":{"id":"irIC7dtKkd9h"},"source":["# NER as Token classification"]},{"cell_type":"markdown","source":["# Load the Model and Tokenizer from bert-base-NER"],"metadata":{"id":"fiL6Of3qmKE9"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import pipeline\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n","model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"],"metadata":{"id":"nzVCXQmtmOy9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a Pipeline from the bert-base-NER Model and Tokenizer"],"metadata":{"id":"U2Q95GgZUxfh"}},{"cell_type":"code","source":["nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"],"metadata":{"id":"IJwZzQHKmcaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare a Text"],"metadata":{"id":"STJcH5aoU-Dj"}},{"cell_type":"code","source":["text = \"Apple Inc. plans to open a new store in San Francisco by January 2024. Tim Cook, the CEO, announced the news yesterday.\""],"metadata":{"id":"RfQiMXUuU_9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Label Tokens with the Tags in the B-I-O Scheme"],"metadata":{"id":"cw8dCvF5VGnH"}},{"cell_type":"code","source":["ner_results = nlp(text)\n","print(ner_results)"],"metadata":{"id":"EXQIhfzImemE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extract the Named Entities"],"metadata":{"id":"2m4lPh3yVdNg"}},{"cell_type":"code","source":["# The code below presumes that ner_results is a list of dictionaries, each representing a token,\n","# arranged in the sequence they appeared in the source sentence.\n","organized_results = {'LOC': [], 'PER': [], 'ORG': [], 'MISC': []}\n","\n","current_entity = None\n","current_words = []\n","\n","for result in ner_results:\n","    entity_type = result['entity'].split('-')[1]\n","    if result['entity'].startswith('B-'):\n","        if current_entity:\n","            organized_results[current_entity].append(' '.join(current_words))\n","        current_entity = entity_type\n","        current_words = [result['word']]\n","    elif result['entity'].startswith('I-') and current_entity == entity_type:\n","        current_words.append(result['word'])\n","\n","# Handle the last entity\n","if current_entity:\n","    organized_results[current_entity].append(' '.join(current_words))\n","\n","# Remove hash symbols from words\n","for key, value in organized_results.items():\n","    organized_results[key] = [' '.join(word.split('##')) for word in value]\n","\n","print(organized_results)\n"],"metadata":{"id":"vcOLJH4TViVR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate a List of Tokens and the Corresponding List of Entity Tags"],"metadata":{"id":"JVap8DzsY0gi"}},{"cell_type":"code","source":["token_list = []\n","tag_list = []\n","for result in ner_results:\n","    token_list.append(result['word'])\n","    tag_list.append(result['entity'])"],"metadata":{"id":"oEKYs4URZKog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_list, tag_list"],"metadata":{"id":"dj0x1DB9Ze7B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5nbf8Nrkd9n"},"source":["# Let Us Test the Model on the CoNLL2003 Data\n"]},{"cell_type":"markdown","metadata":{"id":"mxG4Kxx4kd9o"},"source":["Start by loading the CoNLL2003 dataset from the Datasets library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RE3a0Oc9kd9o"},"outputs":[],"source":["from datasets import load_dataset\n","\n","conll = load_dataset(\"conll2003\")"]},{"cell_type":"markdown","source":["The dataset has been split into train, test, and validation sets:"],"metadata":{"id":"UFSWnNPfHO1D"}},{"cell_type":"code","source":["conll"],"metadata":{"id":"BNt4fl6UwCie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get the features in the datasets:"],"metadata":{"id":"KXm_f0y_Iupl"}},{"cell_type":"code","source":["conll['test'].features"],"metadata":{"id":"kT-ipidov_xG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get the list of tag names:"],"metadata":{"id":"syEB_KQEHd26"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_M9iLD9Fkd9q"},"outputs":[],"source":["tag_names = conll[\"test\"].features[f\"ner_tags\"].feature.names\n","tag_names"]},{"cell_type":"markdown","metadata":{"id":"XHG2PCqIkd9q"},"source":["The letter that prefixes each `ner_tag` indicates the token position of the entity:\n","\n","- `B-` indicates the beginning of an entity.\n","- `I-` indicates a token is contained inside the same entity (for example, the `State` token is a part of an entity like\n","  `Empire State Building`).\n","- `0` indicates the token doesn't correspond to any entity."]},{"cell_type":"markdown","source":["## Test the Model on a Test Data"],"metadata":{"id":"qjCFMklSXgPq"}},{"cell_type":"markdown","source":["Use the instance of index 12 in the test dataset as an example:"],"metadata":{"id":"OvpHCGbC0KeC"}},{"cell_type":"code","source":["example = conll['test'][12]\n","for key in example:\n","    print(key, \":\", example[key])"],"metadata":{"id":"nbNOlVAV0Stf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert the tag ids to tag names to see what entities are recognized:"],"metadata":{"id":"ph1PxjC703pO"}},{"cell_type":"code","source":["example_entities = [tag_names[i] for i in example['ner_tags']]"],"metadata":{"id":"GaaRl2Lx1GRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, w in enumerate(example['tokens']):\n","    print(idx, w, \":\", example_entities[idx])"],"metadata":{"id":"QdR-aj241PZj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the number of original tokens in the given data?"],"metadata":{"id":"QBDKZuM4JkSB"}},{"cell_type":"code","source":["len(example_entities)"],"metadata":{"id":"0WXVJFAb2feG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenize the input by the tokenizer. Set `is_split_into_words=True` so that the given list of tokens can be processed correctly:"],"metadata":{"id":"nlemJ_e70j1I"}},{"cell_type":"code","source":["tokenized_input = tokenizer(example['tokens'], is_split_into_words=True)\n","tokenized_input"],"metadata":{"id":"GKMT7V3MleOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["List the resultant tokens after the tokenization:"],"metadata":{"id":"MFfHlah00oto"}},{"cell_type":"code","source":["tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n","tokens"],"metadata":{"id":"Hn2fikLJl0pz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the number of tokens generated by the tokenizer?"],"metadata":{"id":"_rz8pR_TMKxP"}},{"cell_type":"code","source":["len(tokens)"],"metadata":{"id":"gD-ZL8et5x0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You see there is a mismatch between the result of the tokenization and the given list of tokens. For evaluating the model's performance against the given tags, we need to realign the tokenization result with the given list of tags. Let's see whether the pipeline() could correctly handle the alignment."],"metadata":{"id":"MWzyj6kOMUdv"}},{"cell_type":"markdown","source":["Classify the tokens into recognized entities:"],"metadata":{"id":"-Cy9ZwRn0t_u"}},{"cell_type":"code","source":["ner_results = nlp(example['tokens'])\n","ner_results"],"metadata":{"id":"duit0W9jXzP6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the length of the classification result?"],"metadata":{"id":"-NNRJbtmOoJ7"}},{"cell_type":"code","source":["len(ner_results)"],"metadata":{"id":"feKxeHr6NgYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wonderful! It seems that the classification by pipeline() took care of the tokenization results. Subword tokens were grouped as a single unit if the tokens came from the same word. Now, we can retrieve the list of predictions by using the prediction of the first token in each group."],"metadata":{"id":"rjR-FjEDOt8n"}},{"cell_type":"code","source":["predictions = []\n","for result in ner_results:\n","    if len(result) == 0:\n","        predictions.append('O')\n","    else:\n","        predictions.append(result[0]['entity'])"],"metadata":{"id":"D_vvISIRPZQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(predictions)"],"metadata":{"id":"bNS-K-XGP6SZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the length of the predictions?"],"metadata":{"id":"9dmSqEiqP9sG"}},{"cell_type":"code","source":["len(predictions)"],"metadata":{"id":"tLnTeeABP9OX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great! We have matched predictions and given tags.\n","\n","For the single example, we can see that there are 3 named entity tags in the given list. The model correctly classified 2 of them."],"metadata":{"id":"AOfKQHrEQCxv"}},{"cell_type":"markdown","source":["## Apply the Model to All Test Data"],"metadata":{"id":"ebvGRs_EgZ58"}},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"oSU4eC4mrEYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use the test dataset\n","test = conll['test']"],"metadata":{"id":"dnhTGLw2QzoJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","true_tags_list = []\n","predicted_tags_list = []\n","count = 0 # for test purpose\n","for atest in tqdm(test, desc=str(len(test))):\n","    if count < len(test) + 1:\n","        # add true labels to references\n","        true_tags_list.append([tag_names[id] for id in atest['ner_tags']])\n","\n","        # recognize named entity in a test tokens\n","        test_ner_results = nlp(atest['tokens'])\n","\n","        predicted_tags = []\n","        # extract the predicted tags\n","        for result in test_ner_results:\n","            if len(result) == 0:\n","                predicted_tags.append('O')\n","            else:\n","                predicted_tags.append(result[0]['entity'])\n","\n","        predicted_tags_list.append(predicted_tags)\n","    count += 1"],"metadata":{"id":"iT8Nb7sLie27"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(predicted_tags_list), len(true_tags_list)"],"metadata":{"id":"TyVH7GkUk31T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check the predictions match the true tags"],"metadata":{"id":"uOP8wBfrgfIx"}},{"cell_type":"code","source":["flag = True\n","for idx, apredi in enumerate(predicted_tags_list):\n","    if len(apredi) != len(true_tags_list[idx]):\n","        flag = False\n","        print(idx, \":\", False)\n","if flag:\n","    print(True)"],"metadata":{"id":"_UqzT1DhSnYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0efH3w4kd9s"},"source":["## Evaluate"]},{"cell_type":"code","source":["! pip install -q evaluate seqeval"],"metadata":{"id":"3iySl3n1rzPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1XzRDZvkd9s"},"outputs":[],"source":["import evaluate\n","\n","seqeval = evaluate.load(\"seqeval\")"]},{"cell_type":"markdown","source":["Apply the seqeval to the predicted tags and true tags:"],"metadata":{"id":"85XVxoaJhlu3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZjgOUrTkd9t"},"outputs":[],"source":["results = seqeval.compute(predictions=predicted_tags_list, references=true_tags_list)\n","\n","print(\"precision:\", results[\"overall_precision\"]),\n","print(\"recall:\", results[\"overall_recall\"]),\n","print(\"f1:\", results[\"overall_f1\"]),\n","print(\"accuracy:\", results[\"overall_accuracy\"])"]},{"cell_type":"code","source":[],"metadata":{"id":"zUDAUgrjtw4_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"V100"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}